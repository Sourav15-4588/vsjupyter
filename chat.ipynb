{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd43aff",
   "metadata": {},
   "source": [
    "installing pdf librery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461a4c4",
   "metadata": {},
   "source": [
    "Importing librery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb433dee",
   "metadata": {},
   "source": [
    "Text Extracting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457e1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_tables(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts both text and tables from PDF using pdfplumber\n",
    "    Returns list of pages with content and table info\n",
    "    \"\"\"\n",
    "    print(f\"üìÑ Extracting from: {pdf_path}\")\n",
    "    extracted_data = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        print(f\"Found {total_pages} pages\")\n",
    "        \n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            # Extract text\n",
    "            text = page.extract_text() or \"\"\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            # Extract tables\n",
    "            tables = page.extract_tables()\n",
    "            table_texts = []\n",
    "            \n",
    "            for table_num, table in enumerate(tables):\n",
    "                table_str = f\"\\n[TABLE {table_num + 1}]:\\n\"\n",
    "                for row in table:\n",
    "                    # Clean each cell and join with pipes\n",
    "                    clean_row = [str(cell).strip() if cell else \"\" for cell in row]\n",
    "                    table_str += \" | \".join(clean_row) + \"\\n\"\n",
    "                table_texts.append(table_str)\n",
    "            \n",
    "            # Combine text and tables\n",
    "            full_content = text + \"\\n\" + \"\\n\".join(table_texts)\n",
    "            \n",
    "            extracted_data.append({\n",
    "                'page_number': page_num,\n",
    "                'text': full_content,\n",
    "                'table_count': len(tables),\n",
    "                'has_tables': len(tables) > 0\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úì Page {page_num}: {len(tables)} tables extracted\")\n",
    "    \n",
    "    print(f\"‚úÖ Finished! Extracted {len(extracted_data)} pages\")\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ec11f",
   "metadata": {},
   "source": [
    "Text Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text and tables\n",
    "pdf_path = \"data.pdf\"\n",
    "document_data = extract_text_with_tables(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what we got\n",
    "print(f\"\\nüìä Extraction Summary:\")\n",
    "print(f\"Total pages: {len(document_data)}\")\n",
    "print(f\"Pages with tables: {sum(1 for page in document_data if page['has_tables'])}\")\n",
    "\n",
    "# Show sample of first page\n",
    "if document_data:\n",
    "    first_page = document_data[0]\n",
    "    print(f\"\\nüìù Page {first_page['page_number']} preview:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(first_page['text'][:300] + \"...\" if len(first_page['text']) > 300 else first_page['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298124e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pages that have tables\n",
    "pages_with_tables = [page for page in document_data if page['has_tables']]\n",
    "\n",
    "if pages_with_tables:\n",
    "    print(f\"\\nüîç Found {len(pages_with_tables)} pages with tables:\")\n",
    "    for page in pages_with_tables[:2]:  # Show first 2 pages with tables\n",
    "        print(f\"\\nPage {page['page_number']} ({page['table_count']} tables):\")\n",
    "        print(\"-\" * 30)\n",
    "        # Find the table part in the text\n",
    "        table_part = page['text'].split('[TABLE')[1].split(']')[0] if '[TABLE' in page['text'] else \"No table marker found\"\n",
    "        print(f\"Contains: {table_part}...\")\n",
    "else:\n",
    "    print(\"No tables found in document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b6518",
   "metadata": {},
   "source": [
    "Important Keywords (like budget, debt, or infrastructure details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== IDENTIFY IMPORTANT FINANCIAL INFORMATION ======\n",
    "# (Add this after Task 1 extraction is done, before Task 2)\n",
    "\n",
    "def identify_important_info(document_data):\n",
    "    \"\"\"\n",
    "    Tags important financial information in the extracted data\n",
    "    \"\"\"\n",
    "    print(\"üè∑Ô∏è Tagging important financial information...\")\n",
    "    \n",
    "    financial_keywords = {\n",
    "        'budget': ['budget', 'surplus', 'deficit', 'operating result', 'revenue', 'expense'],\n",
    "        'debt': ['debt', 'borrowing', 'liability', 'interest', 'loan', 'borrowings'],\n",
    "        'infrastructure': ['infrastructure', 'capital works', 'assets', 'property', 'plant', 'equipment'],\n",
    "        'taxation': ['tax', 'taxation', 'gsp', 'revenue', 'tax burden'],\n",
    "        'superannuation': ['superannuation', 'pension', 'retirement', 'liabilities'],\n",
    "        'financial_policy': ['financial policy', 'objective', 'strategy', 'principle'],\n",
    "        'service_delivery': ['service delivery', 'health', 'education', 'community']\n",
    "    }\n",
    "    \n",
    "    tagged_data = []\n",
    "    \n",
    "    for page in document_data:\n",
    "        text_lower = page['text'].lower()\n",
    "        page_tags = []\n",
    "        \n",
    "        # Check which financial topics are on this page\n",
    "        for topic, keywords in financial_keywords.items():\n",
    "            if any(keyword in text_lower for keyword in keywords):\n",
    "                page_tags.append(topic)\n",
    "        \n",
    "        # Add tags to page data\n",
    "        tagged_page = page.copy()\n",
    "        tagged_page['financial_topics'] = page_tags\n",
    "        tagged_data.append(tagged_page)\n",
    "        \n",
    "        if page_tags:\n",
    "            print(f\"‚úì Page {page['page_number']}: {', '.join(page_tags)}\")\n",
    "    \n",
    "    return tagged_data\n",
    "\n",
    "# Run the tagging\n",
    "print(\"üîç Identifying important financial information...\")\n",
    "document_data = identify_important_info(document_data)\n",
    "\n",
    "# Show summary\n",
    "print(f\"\\nüìä Important topics found:\")\n",
    "all_topics = set()\n",
    "for page in document_data:\n",
    "    all_topics.update(page['financial_topics'])\n",
    "\n",
    "print(f\"Financial concepts identified: {', '.join(sorted(all_topics))}\")\n",
    "\n",
    "# Verify the new structure\n",
    "print(f\"\\nüßÆ Sample page structure now includes 'financial_topics':\")\n",
    "if document_data:\n",
    "    print(f\"Page 1 keys: {list(document_data[0].keys())}\")\n",
    "    print(f\"Page 1 topics: {document_data[0]['financial_topics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5a8a4",
   "metadata": {},
   "source": [
    "Task 2 \n",
    "Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Chunking Function\n",
    "def chunk_document_text(extracted_data, sentences_per_chunk=4):\n",
    "    \"\"\"\n",
    "    Processes extracted PDF text into smaller chunks for better search.\n",
    "    Each chunk contains sentences from the same page with financial tags.\n",
    "    \"\"\"\n",
    "    print(\"‚úÇÔ∏è Chunking document text into search-optimized pieces...\")\n",
    "    chunks_with_metadata = []\n",
    "    \n",
    "    for page_data in extracted_data:\n",
    "        page_num = page_data['page_number']\n",
    "        text = page_data['text']\n",
    "        financial_topics = page_data['financial_topics']\n",
    "        \n",
    "        # Split text into sentences (better approach for financial documents)\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "        \n",
    "        # Create chunks of 3-5 sentences (optimal for financial Q&A)\n",
    "        for i in range(0, len(sentences), sentences_per_chunk):\n",
    "            chunk_sentences = sentences[i:i + sentences_per_chunk]\n",
    "            chunk_text = ' '.join(chunk_sentences).strip()\n",
    "            \n",
    "            if chunk_text and len(chunk_text) > 50:  # Skip very short chunks\n",
    "                chunks_with_metadata.append({\n",
    "                    'text': chunk_text,\n",
    "                    'page_number': page_num,\n",
    "                    'financial_topics': financial_topics,\n",
    "                    'chunk_id': f\"page{page_num}_chunk{i//sentences_per_chunk + 1}\",\n",
    "                    'has_tables': page_data['has_tables'],\n",
    "                    'table_count': page_data['table_count']\n",
    "                })\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(chunks_with_metadata)} chunks from {len(extracted_data)} pages\")\n",
    "    return chunks_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process your extracted text into optimized chunks\n",
    "document_chunks = chunk_document_text(document_data)\n",
    "\n",
    "print(f\"\\nüìä Chunking completed!\")\n",
    "print(f\"Total chunks created: {len(document_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine what we created\n",
    "print(\"üîç Sample of created chunks (with financial tags):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show chunks from different financial topics\n",
    "sample_chunks = []\n",
    "for chunk in document_chunks:\n",
    "    if chunk['financial_topics']:  # Only show chunks with financial tags\n",
    "        sample_chunks.append(chunk)\n",
    "    if len(sample_chunks) >= 3:  # Show 3 samples\n",
    "        break\n",
    "\n",
    "for i, chunk in enumerate(sample_chunks):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Page {chunk['page_number']} | Topics: {chunk['financial_topics']}\")\n",
    "    print(f\"Content: {chunk['text'][:100]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880407de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chunk distribution by financial topic\n",
    "print(\"üìà Chunk distribution by financial topic:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "topic_distribution = {}\n",
    "for chunk in document_chunks:\n",
    "    for topic in chunk['financial_topics']:\n",
    "        topic_distribution[topic] = topic_distribution.get(topic, 0) + 1\n",
    "\n",
    "# Sort by count (most common first)\n",
    "for topic, count in sorted(topic_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{topic}: {count} chunks\")\n",
    "\n",
    "print(f\"\\nChunks with tables: {sum(1 for chunk in document_chunks if chunk['has_tables'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick quality check\n",
    "print(\"\\nüß™ Quality Check:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check average chunk length\n",
    "avg_length = sum(len(chunk['text']) for chunk in document_chunks) / len(document_chunks)\n",
    "print(f\"Average chunk length: {avg_length:.0f} characters\")\n",
    "\n",
    "# Check chunks are properly tagged\n",
    "untagged_chunks = sum(1 for chunk in document_chunks if not chunk['financial_topics'])\n",
    "print(f\"Untagged chunks: {untagged_chunks}\")\n",
    "\n",
    "# Show a table-containing chunk example\n",
    "table_chunks = [chunk for chunk in document_chunks if chunk['has_tables']]\n",
    "if table_chunks:\n",
    "    print(f\"\\nExample table chunk (Page {table_chunks[0]['page_number']}):\")\n",
    "    print(table_chunks[0]['text'][:150] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9acbd",
   "metadata": {},
   "source": [
    "Saving the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== SAVE PROCESSED DATA ======\n",
    "import pickle\n",
    "\n",
    "def save_processed_data(chunks, filename=\"financial_chunks.pkl\"):\n",
    "    \"\"\"Save processed chunks to disk\"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    print(f\"üíæ Saved {len(chunks)} chunks to {filename}\")\n",
    "\n",
    "# Save your hard work!\n",
    "save_processed_data(document_chunks)\n",
    "\n",
    "# Optional: Quick verification\n",
    "print(f\"üì¶ Sample chunk structure:\")\n",
    "print(f\"Keys: {list(document_chunks[0].keys())}\")\n",
    "print(f\"First topic: {document_chunks[0]['financial_topics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5e000",
   "metadata": {},
   "source": [
    "Making Vector Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe838ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ca79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model (creates numerical representations of text)\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ Embedding model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ededdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb --upgrade\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d08ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize embedding model\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ Embedding model loaded!\")\n",
    "\n",
    "# Initialize ChromaDB with simpler configuration\n",
    "print(\"üóÑÔ∏è Initializing vector database...\")\n",
    "client = chromadb.Client()  # Use default in-memory client for simplicity\n",
    "\n",
    "# Create collection\n",
    "collection = client.create_collection(\n",
    "    name=\"financial_policy\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector database initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a30211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_vector_db(chunks):\n",
    "    \"\"\"Prepare chunks for insertion into vector database\"\"\"\n",
    "    print(\"üì¶ Preparing data for vector database...\")\n",
    "    \n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        texts.append(chunk['text'])\n",
    "        metadatas.append({\n",
    "            'page_number': chunk['page_number'],\n",
    "            'financial_topics': str(chunk['financial_topics']),  # Convert to string for ChromaDB\n",
    "            'chunk_id': chunk['chunk_id'],\n",
    "            'has_tables': chunk['has_tables']\n",
    "        })\n",
    "        ids.append(f\"chunk_{i}\")\n",
    "    \n",
    "    return texts, metadatas, ids\n",
    "\n",
    "# Prepare your chunks\n",
    "texts, metadatas, ids = prepare_for_vector_db(document_chunks)\n",
    "print(f\"‚úÖ Prepared {len(texts)} chunks for database insertion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† Creating embeddings... (this may take a moment)\")\n",
    "# Process in batches to avoid memory issues\n",
    "batch_size = 50\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "    batch_embeddings = embedding_model.encode(batch_texts).tolist()\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    print(f\"Processed batch {i//batch_size + 1}/{(len(texts)//batch_size)+1}\")\n",
    "\n",
    "print(\"üíæ Adding chunks to vector database...\")\n",
    "collection.add(\n",
    "    embeddings=all_embeddings,\n",
    "    documents=texts,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector database populated!\")\n",
    "print(f\"üìä Total chunks stored: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a459b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_semantic_search(query, n_results=3):\n",
    "    \"\"\"Test semantic search with a sample query\"\"\"\n",
    "    print(f\"\\nüîç Testing search: '{query}'\")\n",
    "    \n",
    "    # Create embedding for the query\n",
    "    query_embedding = embedding_model.encode([query]).tolist()\n",
    "    \n",
    "    try:\n",
    "        # Search the database\n",
    "        results = collection.query(\n",
    "            query_embeddings=query_embedding,\n",
    "            n_results=n_results,\n",
    "            include=['metadatas', 'documents', 'distances']\n",
    "        )\n",
    "        \n",
    "        print(f\"Found {len(results['documents'][0])} results:\")\n",
    "        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "            print(f\"\\n{i+1}. Page {metadata['page_number']} (Similarity: {1 - results['distances'][0][i]:.3f})\")\n",
    "            print(f\"   Topics: {metadata['financial_topics']}\")\n",
    "            print(f\"   Content: {doc[:100]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test searches\n",
    "test_queries = [\n",
    "    \"budget deficit 2005\",\n",
    "    \"debt levels\",\n",
    "    \"infrastructure investment\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    test_semantic_search(query)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478c731",
   "metadata": {},
   "source": [
    "Searching Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951981c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "print(\"üß™ Final verification:\")\n",
    "print(f\"Collection name: {collection.name}\")\n",
    "print(f\"Total chunks: {collection.count()}\")\n",
    "\n",
    "qa = input(\"Enter your query: \")\n",
    "\n",
    "# Try a simple search\n",
    "results = collection.query(\n",
    "    query_texts=[qa],\n",
    "    n_results=2\n",
    ")\n",
    "print(f\"Sample search works: {len(results['documents'][0])} results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1017f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Query Input Cell (Run this cell to get input)\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create input widget\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your financial query...',\n",
    "    description='Query:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "search_button = widgets.Button(description=\"Search\", button_style='success')\n",
    "\n",
    "# Display widgets\n",
    "display(query_input)\n",
    "display(search_button)\n",
    "\n",
    "# Store results\n",
    "search_results = []\n",
    "\n",
    "def on_search_clicked(b):\n",
    "    query = query_input.value.strip()\n",
    "    if query:\n",
    "        print(f\"üîç Searching for: '{query}'\")\n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=3,\n",
    "            include=['metadatas', 'documents', 'distances']\n",
    "        )\n",
    "        search_results.append(results)\n",
    "        display_search_results(results)\n",
    "    else:\n",
    "        print(\"‚ùå Please enter a query\")\n",
    "\n",
    "search_button.on_click(on_search_clicked)\n",
    "\n",
    "def display_search_results(results):\n",
    "    \"\"\"Display search results nicely\"\"\"\n",
    "    from IPython.display import HTML, display\n",
    "    \n",
    "    if results and results['documents']:\n",
    "        html_output = f\"<h3>üìÑ Found {len(results['documents'][0])} results:</h3>\"\n",
    "        \n",
    "        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "            similarity = 1 - results['distances'][0][i] if results['distances'] else 0\n",
    "            html_output += f\"\"\"\n",
    "            <div style='border: 1px solid #ccc; padding: 10px; margin: 10px; border-radius: 5px;'>\n",
    "                <b>{i+1}. üìç Page {metadata['page_number']}</b> (Similarity: {similarity:.3f})<br>\n",
    "                <small>üè∑Ô∏è Topics: {metadata['financial_topics']}</small><br>\n",
    "                <p>{doc[:150]}...</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        display(HTML(html_output))\n",
    "    else:\n",
    "        print(\"‚ùå No results found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
